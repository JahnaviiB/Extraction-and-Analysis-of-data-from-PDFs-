{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ------------------------------------- 302.2/302.2 kB 19.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Jahnavi\n",
      "[nltk_data]     Boya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.095, 'neu': 0.865, 'pos': 0.04, 'compound': -0.9992}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "pdf_doc = pdfplumber.open(r'C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\combined.pdf')\n",
    "pdf_text = ''\n",
    "for single_page in pdf_doc.pages:\n",
    "    pdf_text += single_page.extract_text()\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    # Create a SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Get the sentiment scores for the text\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "    # Determine the sentiment based on the compound score\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "\n",
    "    return sentiment, sentiment_scores\n",
    "\n",
    "text_to_analyze = pdf_text\n",
    "sentiment, scores = perform_sentiment_analysis(text_to_analyze)\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Sentiment Scores: {scores}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Your COVID-19 test result\n",
      "NEGATIVE\n",
      "A negative result for this test means that SARS-CoV-2 RNA (the cause of\n",
      "COVID-19) was not detected in the collection sample.\n",
      "What does it mean to have a negative test result?\n",
      "A negative test result does not completely rule out being infected with COVID-19.\n",
      "If you test negative for COVID-19, this means the virus was not detected at the time your\n",
      "specimen was collected. It is still possible that you were very early in your infection at the time of\n",
      "your specimen collection and that you could test positive later.\n",
      "Also, you could be exposed later and still develop the illness. For all these reasons, it is\n",
      "important to follow CDC guidance, including but not limited to frequent hand washing, social\n",
      "distancing, wearing a face covering, covering coughs and sneezes, monitoring symptoms, and\n",
      "cleaning and disinfectant of frequently touched surfaces - even after a negative test result.\n",
      "Test information\n",
      "Patient's name Collection date\n",
      "Cassidy Cichowicz November 27, 2020 at 12:10 PM ET\n",
      "Patient's date of birth Collection location\n",
      "June 18, 1994 220 FAIRVIEW LANE, CARSON CITY, NV\n",
      "89701\n",
      "Test type\n",
      "SARS-COV-2 RNA, QL, RT PCR (COVID-19)\n",
      "Provider\n",
      "BAILEY JENNIFER\n",
      "MinuteClinic contact information\n",
      "Customer Service: (886) 389-2727Your COVID-19 test result\n",
      "NEGATIVE\n",
      "A negative result for this test means that SARS-CoV-2 RNA (the cause of\n",
      "COVID-19) was not detected in the collection sample.\n",
      "Legal Disclaimer\n",
      "A Not Detected (negative) test result for this test means that SARS- CoV-2 RNA was not present in the specimen\n",
      "above the limit of detection. A negative result does not rule out the possibility of COVID-19 and should not be used\n",
      "as the sole basis for treatment or patient management decisions. If COVID-19 is still suspected, based on exposure\n",
      "history together with other clinical findings, re-testing should be considered in consultation with public health\n",
      "authorities. Laboratory test results should always be considered in the context of clinical observations and\n",
      "epidemiological data in making a final diagnosis and patient management decisions.\n",
      "Please review the \"Fact Sheets\" and FDA authorized labeling available for health care providers and patients using\n",
      "the following websites:\n",
      "https://www.questdiagnostics.com/home/Covid-19/HCP/QuestIVD/fact-sheet.html\n",
      "https://www.questdiagnostics.com/home/Covid-19/Patients/QuestIVD/fact-sheet.html\n",
      "This test has been authorized by the FDA under an Emergency Use Authorization (EUA) for use by authorized\n",
      "laboratories.\n",
      "Due to the current public health emergency, Quest Diagnostics is receiving a high volume of samples from a wide\n",
      "variety of swabs and media for COVID-19 testing. In order to serve patients during this public health crisis, samples\n",
      "from appropriate clinical sources are being tested. Negative test results derived from specimens received in non-\n",
      "commercially manufactured viral collection and transport media, or in media and sample collection kits not yet\n",
      "authorized by FDA for COVID-19 testing should be cautiously evaluated and the patient potentially subjected to\n",
      "extra precautions such as additional clinical monitoring, including collection of an additional specimen.\n",
      "Methodology: Nucleic Acid Amplification Test (NAAT) includes RT-PCR or TMA\n",
      "Additional information about COVID-19 can be found at the Quest Diagnostics website: www.QuestDiagnostics.com/\n",
      "Covid19.\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.109, 'neu': 0.855, 'pos': 0.036, 'compound': -0.9914}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Jahnavi\n",
      "[nltk_data]     Boya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "pdf_doc = pdfplumber.open(r'C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\1.pdf')\n",
    "pdf_text = ''\n",
    "for single_page in pdf_doc.pages:\n",
    "    pdf_text += single_page.extract_text()\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    # Create a SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Get the sentiment scores for the text\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "    # Determine the sentiment based on the compound score\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "\n",
    "    return sentiment, sentiment_scores\n",
    "\n",
    "# Example usage\n",
    "text_to_analyze = pdf_text\n",
    "sentiment, scores = perform_sentiment_analysis(text_to_analyze)\n",
    "\n",
    "print(f\"Text: {text_to_analyze}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Sentiment Scores: {scores}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.04, 'neu': 0.96, 'pos': 0.0, 'compound': -0.3818}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Jahnavi\n",
      "[nltk_data]     Boya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "pdf_doc = pdfplumber.open(r'C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\entities_report.pdf')\n",
    "pdf_text = ''\n",
    "for single_page in pdf_doc.pages:\n",
    "    pdf_text += single_page.extract_text()\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    # Create a SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Get the sentiment scores for the text\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "    # Determine the sentiment based on the compound score\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "\n",
    "    return sentiment, sentiment_scores\n",
    "\n",
    "\n",
    "text_to_analyze = pdf_text\n",
    "sentiment, scores = perform_sentiment_analysis(text_to_analyze)\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Sentiment Scores: {scores}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.4)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.3 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymupdf) (1.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The domain in the PDF is: https://www.labcorp.com/COVID19\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "def extract_domain_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text_list = []\n",
    "\n",
    "    # Iterate over all pages in the PDF\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the text from the current page\n",
    "        page = pdf_document[page_number]\n",
    "        text = page.get_text(\"text\")\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Convert the list of text into a single string\n",
    "    pdf_text = \" \".join(text_list)\n",
    "\n",
    "    # Search for the domain in the extracted text\n",
    "    domain_start = pdf_text.find(\"http://\")\n",
    "    if domain_start == -1:\n",
    "        domain_start = pdf_text.find(\"https://\")\n",
    "\n",
    "    if domain_start != -1:\n",
    "        # Extract the domain from the URL\n",
    "        domain_end = pdf_text.find(\" \", domain_start)\n",
    "        if domain_end == -1:\n",
    "            domain_end = len(pdf_text)\n",
    "\n",
    "        domain = pdf_text[domain_start:domain_end]\n",
    "        return domain\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\3.pdf\"\n",
    "domain = extract_domain_from_pdf(pdf_path)\n",
    "\n",
    "if domain:\n",
    "    print(f\"The domain in the PDF is: {domain}\")\n",
    "else:\n",
    "    print(\"No domain found in the PDF.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.3 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyMuPDF) (1.23.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "     ---------------------------------------- 44.1/44.1 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: click in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jahnavi boya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 scipy-1.11.3 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF scikit-learn nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jahnavi\n",
      "[nltk_data]     Boya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jahnavi\n",
      "[nltk_data]     Boya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix for the Combined PDF:\n",
      "[[0.01862259 0.02793388 0.02793388 0.07449035 0.02793388 0.09311294\n",
      "  0.02793388 0.01862259 0.01862259 0.08380164 0.01862259 0.02793388\n",
      "  0.02793388 0.01862259 0.10242423 0.13035811 0.02793388 0.01862259\n",
      "  0.01862259 0.01862259 0.01862259 0.02793388 0.02793388 0.08380164\n",
      "  0.02793388 0.01862259 0.01862259 0.01862259 0.03724517 0.01862259\n",
      "  0.01862259 0.02793388 0.02793388 0.01862259 0.02793388 0.01862259\n",
      "  0.01862259 0.02793388 0.13035811 0.00931129 0.04655647 0.1489807\n",
      "  0.02793388 0.01862259 0.01862259 0.00931129 0.02793388 0.06517906\n",
      "  0.02793388 0.01862259 0.04655647 0.02793388 0.04655647 0.02793388\n",
      "  0.02793388 0.01862259 0.03724517 0.02793388 0.03724517 0.01862259\n",
      "  0.01862259 0.01862259 0.01862259 0.17691458 0.02793388 0.03724517\n",
      "  0.02793388 0.01862259 0.08380164 0.16760329 0.04655647 0.00931129\n",
      "  0.02793388 0.01862259 0.02793388 0.04655647 0.05586776 0.03724517\n",
      "  0.05586776 0.01862259 0.01862259 0.01862259 0.02793388 0.05586776\n",
      "  0.02793388 0.02793388 0.01862259 0.09311294 0.02793388 0.02793388\n",
      "  0.01862259 0.02793388 0.07449035 0.04655647 0.01862259 0.01862259\n",
      "  0.02793388 0.02793388 0.01862259 0.01862259 0.02793388 0.01862259\n",
      "  0.01862259 0.01862259 0.01862259 0.02793388 0.11173552 0.02793388\n",
      "  0.04655647 0.01862259 0.02793388 0.01862259 0.01862259 0.01862259\n",
      "  0.01862259 0.01862259 0.02793388 0.02793388 0.01862259 0.01862259\n",
      "  0.10242423 0.01862259 0.01862259 0.02793388 0.00931129 0.04655647\n",
      "  0.11173552 0.01862259 0.01862259 0.02793388 0.01862259 0.03724517\n",
      "  0.00931129 0.01862259 0.00931129 0.02793388 0.01862259 0.04655647\n",
      "  0.02793388 0.12104682 0.02793388 0.02793388 0.02793388 0.02793388\n",
      "  0.01862259 0.01862259 0.02793388 0.01862259 0.05586776 0.08380164\n",
      "  0.01862259 0.04655647 0.04655647 0.01862259 0.03724517 0.02793388\n",
      "  0.01862259 0.01862259 0.01862259 0.02793388 0.01862259 0.01862259\n",
      "  0.03724517 0.01862259 0.02793388 0.03724517 0.02793388 0.02793388\n",
      "  0.01862259 0.07449035 0.05586776 0.01862259 0.00931129 0.02793388\n",
      "  0.01862259 0.00931129 0.03724517 0.11173552 0.01862259 0.02793388\n",
      "  0.01862259 0.08380164 0.28865011 0.01862259 0.02793388 0.07449035\n",
      "  0.01862259 0.01862259 0.01862259 0.01862259 0.02793388 0.02793388\n",
      "  0.02793388 0.17691458 0.06517906 0.01862259 0.02793388 0.02793388\n",
      "  0.02793388 0.02793388 0.05586776 0.05586776 0.01862259 0.02793388\n",
      "  0.04655647 0.01862259 0.01862259 0.01862259 0.02793388 0.01862259\n",
      "  0.02793388 0.02793388 0.01862259 0.01862259 0.05586776 0.01862259\n",
      "  0.03724517 0.01862259 0.07449035 0.01862259 0.02793388 0.00931129\n",
      "  0.02793388 0.02793388 0.00931129 0.11173552 0.02793388 0.00931129\n",
      "  0.02793388 0.00931129 0.27002752 0.03724517 0.01862259 0.02793388\n",
      "  0.02793388 0.07449035 0.02793388 0.01862259 0.02793388 0.03724517\n",
      "  0.08380164 0.03724517 0.02793388 0.02793388 0.01862259 0.01862259\n",
      "  0.02793388 0.01862259 0.02793388 0.01862259 0.01862259 0.01862259\n",
      "  0.02793388 0.01862259 0.15829199 0.01862259 0.02793388 0.05586776\n",
      "  0.02793388 0.01862259 0.00931129 0.01862259 0.01862259 0.01862259\n",
      "  0.08380164 0.02793388 0.02793388 0.49349857 0.01862259 0.13035811\n",
      "  0.02793388 0.08380164 0.06517906 0.04655647 0.01862259 0.01862259\n",
      "  0.01862259 0.03724517 0.01862259 0.00931129 0.02793388 0.02793388\n",
      "  0.09311294 0.01862259 0.01862259 0.01862259 0.02793388 0.01862259\n",
      "  0.07449035 0.00931129 0.02793388 0.01862259 0.01862259 0.01862259\n",
      "  0.02793388 0.01862259 0.01862259 0.02793388 0.02793388 0.01862259\n",
      "  0.02793388]]\n",
      "1\n",
      "\n",
      "Feature Names:\n",
      "['abnormal' 'account' 'acct' 'acid' 'act' 'additional' 'age' 'also'\n",
      " 'always' 'amplification' 'appropriate' 'approved' 'assay' 'authorities'\n",
      " 'authorization' 'authorized' 'available' 'bailey' 'based' 'basis' 'birth'\n",
      " 'bn' 'branch' 'burlington' 'call' 'care' 'carson' 'cassidy' 'cause'\n",
      " 'cautiously' 'cdc' 'characteristics' 'church' 'cichowicz' 'circumstances'\n",
      " 'city' 'cleaning' 'cleared' 'clinical' 'clinically' 'collected'\n",
      " 'collection' 'comments' 'commercially' 'completely' 'components'\n",
      " 'confidential' 'considered' 'consistent' 'consultation' 'contact'\n",
      " 'contains' 'context' 'control' 'corporation' 'coughs' 'could' 'court'\n",
      " 'covering' 'crisis' 'current' 'customer' 'data' 'date' 'day' 'decisions'\n",
      " 'declaration' 'derived' 'details' 'detected' 'detection' 'determine'\n",
      " 'determined' 'develop' 'developed' 'diagnosis' 'diagnostic' 'diagnostics'\n",
      " 'dir' 'disclaimer' 'disinfectant' 'distancing' 'dob' 'document' 'due'\n",
      " 'duration' 'early' 'emergency' 'entered' 'enterprise' 'epidemiological'\n",
      " 'error' 'et' 'eua' 'evaluated' 'even' 'exist' 'expect' 'exposed'\n",
      " 'exposure' 'exposures' 'extra' 'face' 'fact' 'fairview' 'false' 'fda'\n",
      " 'federal' 'final' 'findings' 'flag' 'follow' 'following' 'found'\n",
      " 'frequent' 'frequently' 'gender' 'general' 'guidance' 'hand' 'health'\n",
      " 'high' 'history' 'holdings' 'hospitalized' 'https' 'id' 'illness'\n",
      " 'important' 'include' 'includes' 'including' 'inconsistent'\n",
      " 'indeterminate' 'indicated' 'individual' 'infected' 'infection' 'info'\n",
      " 'information' 'inquiries' 'interval' 'issued' 'items' 'jennifer' 'june'\n",
      " 'justifying' 'kits' 'lab' 'labcorp' 'labeling' 'laboratories'\n",
      " 'laboratory' 'lane' 'later' 'law' 'legal' 'limit' 'limited' 'local'\n",
      " 'location' 'making' 'management' 'manufactured' 'master' 'may'\n",
      " 'mcleansville' 'md' 'mean' 'means' 'media' 'methodology' 'mild'\n",
      " 'millstream' 'minuteclinic' 'moderate' 'monitoring' 'naa' 'naat'\n",
      " 'nagendra' 'name' 'nc' 'negative' 'november' 'npi' 'nucleic' 'nv'\n",
      " 'observations' 'options' 'order' 'ordered' 'ordering' 'page' 'patient'\n",
      " 'patients' 'pcr' 'performance' 'performed' 'phd' 'phone' 'physician'\n",
      " 'please' 'pm' 'positive' 'possibility' 'possible' 'potentially'\n",
      " 'precautions' 'presence' 'present' 'private' 'protected' 'provider'\n",
      " 'providers' 'public' 'ql' 'quest' 'reasons' 'received' 'receiving'\n",
      " 'recent' 'recollect' 'reference' 'referring' 'reliably' 'report'\n",
      " 'reported' 'required' 'reserved' 'resources' 'result' 'results' 'review'\n",
      " 'revoked' 'rights' 'rna' 'road' 'rt' 'rte' 'rule' 'sample' 'samples'\n",
      " 'sanjai' 'section' 'serve' 'service' 'shedding' 'sheets' 'signs'\n",
      " 'sneezes' 'social' 'sole' 'sooner' 'sources' 'specimen' 'specimens'\n",
      " 'state' 'still' 'street' 'subjected' 'submitted' 'surfaces' 'suspected'\n",
      " 'swabs' 'symptoms' 'tat' 'terminated' 'test' 'tested' 'testing'\n",
      " 'testmaster' 'tests' 'time' 'tma' 'together' 'touched' 'transport'\n",
      " 'treatment' 'type' 'unable' 'units' 'unless' 'use' 'used' 'using'\n",
      " 'variety' 'version' 'viral' 'virus' 'visit' 'vitro' 'volume' 'washing'\n",
      " 'wearing' 'website' 'websites' 'wide' 'without' 'would' 'yet' 'york']\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Step 2: Preprocess the text (tokenization and stopword removal)\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Step 3: Apply TF-IDF\n",
    "def apply_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\combined.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "preprocessed_text = preprocess_text(pdf_text)\n",
    "tfidf_matrix, feature_names = apply_tfidf([preprocessed_text])\n",
    "\n",
    "\n",
    "print(\"TF-IDF Matrix for the Combined PDF:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "tfdi = tfidf_matrix.toarray()\n",
    "print(len(tfdi))\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix for the entity report:\n",
      "[[0.08304548 0.16609096 0.08304548 0.08304548 0.08304548 0.08304548\n",
      "  0.08304548 0.08304548 0.16609096 0.08304548 0.08304548 0.08304548\n",
      "  0.08304548 0.08304548 0.08304548 0.16609096 0.08304548 0.08304548\n",
      "  0.08304548 0.08304548 0.08304548 0.08304548 0.8304548  0.08304548\n",
      "  0.16609096 0.08304548 0.08304548 0.08304548 0.08304548 0.08304548\n",
      "  0.08304548 0.08304548 0.08304548 0.08304548]]\n",
      "1\n",
      "\n",
      "Feature Names:\n",
      "['authorization' 'cardinal' 'carson' 'cassidy' 'cdc' 'cichowicz' 'city'\n",
      " 'customer' 'date' 'diagnostics' 'emergency' 'et' 'eua' 'fact' 'fda' 'gpe'\n",
      " 'june' 'law' 'minuteclinic' 'naat' 'november' 'nv' 'org' 'pcr' 'person'\n",
      " 'pm' 'ql' 'quest' 'rt' 'service' 'sheets' 'time' 'tma' 'use']\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Step 2: Preprocess the text (tokenization and stopword removal)\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Step 3: Apply TF-IDF\n",
    "def apply_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\Jahnavi Boya\\OneDrive\\Desktop\\Masters Project\\PDF\\entities_report.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "preprocessed_text = preprocess_text(pdf_text)\n",
    "tfidf_matrix, feature_names = apply_tfidf([preprocessed_text])\n",
    "\n",
    "\n",
    "print(\"TF-IDF Matrix for the entity report:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "tfdi = tfidf_matrix.toarray()\n",
    "print(len(tfdi))\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
